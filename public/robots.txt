# robots.txt - Anti-Scraping Protection
# Blocks all bots and crawlers from indexing blog content

User-agent: *
Disallow: /blog/
Disallow: /category/
Disallow: /admin/
Disallow: /api/

# Explicitly block known scrapers
User-agent: Googlebot
Disallow: /blog/
Disallow: /category/

User-agent: Bingbot
Disallow: /blog/
Disallow: /category/

User-agent: Slurp
Disallow: /blog/
Disallow: /category/

User-agent: DuckDuckBot
Disallow: /blog/
Disallow: /category/

User-agent: Baiduspider
Disallow: /blog/
Disallow: /category/

User-agent: YandexBot
Disallow: /blog/
Disallow: /category/

User-agent: Sogou
Disallow: /blog/
Disallow: /category/

User-agent: Exabot
Disallow: /blog/
Disallow: /category/

User-agent: facebot
Disallow: /blog/
Disallow: /category/

User-agent: ia_archiver
Disallow: /blog/
Disallow: /category/

# Block all scrapers
User-agent: *
Crawl-delay: 86400  # 24 hour delay (effectively blocks)

# Sitemap (optional - remove if you don't want indexing at all)
# Sitemap: https://satyamparmar-dev.github.io/satyamparmar/sitemap.xml


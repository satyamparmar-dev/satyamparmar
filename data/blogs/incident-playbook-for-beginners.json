{"title":"Incident Playbook for Beginners: Real-World Monitoring and Troubleshooting Stories","slug":"incident-playbook-for-beginners","date":"2025-10-30","author":"Satyam Parmar","tags":["Backend Engineering","Incident Response","Monitoring","SRE","Troubleshooting","On-call"],"excerpt":"A story-driven, plain English incident playbook for new backend & SRE engineers. Find, fix, and prevent outages with empathy and practical steps.","content":"# Incident Playbook for Beginners\n\n> Calmly troubleshoot slowdowns, errors, and outages — even at 3AM. These are real stories, explained in language anyone new to on-call can follow.\n\n\n## 🚦 \"Why does my API suddenly slow down?\"\n\n**Symptom:**\n> Out of the blue, your API takes 2 seconds instead of 100ms to respond. Users say: “It’s randomly slow.” No errors in your logs.\n\n### Where to start\n- Check CPU, memory, database, network traffic metrics.\n- Open tracing tools (Datadog, Grafana) — see where the request lags.\n- Is your API waiting on another service?\n- Are too many requests hitting you at once?\n\n### What’s really happening?\n1. User makes a request.\n2. You call a DB or another API — it’s sometimes slow.\n3. Your service waits, builds up more requests.\n4. Finally replies… late.\n\n### Why?\n- DB is backed up, so your app waits for a free connection.\n- External API is just slow.\n- Too many requests in your queue.\n- App is paused for GC (memory cleanup).\n\n### Quick checks\n- Is DB or an external service overloaded?\n- Look for latency spikes in dashboards.\n- Shell command:\n  ```bash\n  kubectl top pod\n  ```\n\n### Quick fixes\n- Set timeouts on outgoing calls.\n- Add caching for frequent requests.\n- Temporarily scale up your service.\n- Restart any stuck pod.\n\n### Prevent this next time\n- Alert on high latency.\n- Watch dependency health.\n- Do real-traffic load simulations before release.\n\n---\n\n## 🚦 \"Why are users seeing 500 errors, but my app logs are empty?\"\n\n**Symptom:**\n> Users see a 500 error. You don’t see any error logged.\n\n### Where to check\n- Error before your app? (Check load balancer, gateway, firewall.)\n- Did your container restart before it could log?\n- Are logs being sampled or filtered too aggressively?\n\n### What’s really happening?\n1. LB gets user request, forwards to you.\n2. Your service crashes — no log gets written.\n3. LB returns 500 to the user.\n\n### Why?\n- Crash before log flush.\n- Proxy returns error for you.\n- Log level too high for error details.\n- App killed by OS for OOM (out-of-memory).\n\n### Quick checks\n- `kubectl describe pod <pod>`\n- `kubectl get events`\n- `dmesg | grep -i oom`\n- Compare Request IDs across system logs.\n- Temporarily lower log level.\n\n### Quick fixes\n- Roll back to last stable version.\n- Restart pods, give more memory.\n- Add try/catch + log in all routes.\n\n### Prevent this next time\n- Always include correlation/request IDs everywhere.\n- Synthetic user tests.\n- Make error logging more robust by default.\n\n---\n\n## 🚦 \"99% uptime, but users still complain\"\n\n**Symptom:**\n> Dashboards: “100% uptime.” Users: “The site’s down/slow!”\n\n### Where to check\n- Are your health checks only hitting `/health`?\n- Regional/ISP issue?\n- Are users using different API endpoints?\n- Is frontend actually failing, not backend?\n\n### What’s really happening?\n1. Health check always returns OK, but real routes fail.\n2. Problems in certain regions or features, not covered by synthetic checks.\n\n### Why?\n- Health check is too shallow — just “is server alive.”\n- Downstream service is the one broken.\n- Metrics average out and hide regional outages.\n\n### Quick checks\n- Test real endpoints from multiple regions.\n- Compare synthetic vs real user traffic in dashboards.\n- Review readiness/liveness probes for depth.\n\n### Quick fixes\n- Make health checks exercise real app dependencies.\n- Add geographic dashboards.\n- Reroute or throttle bad regions.\n\n### Prevent this next time\n- RUM (real user monitoring) everywhere.\n- Define uptime as “real requests succeed.”\n- SLOs/SLAs based on end-user outcome.\n\n---\n\n## 🚦 \"Suddenly 100s of alerts: what now?\"\n\n**Symptom:**\n> You’re paged at 3AM, alerts flooding in. Every service looks down. What now?\n\n### Where to check\n- Which alert/service failed FIRST?\n- Recent deploy or infra change?\n- Any dependencies that triggered cascade failures?\n\n### What’s really happening?\n1. Something core (DB, major service) failed.\n2. All dependents start to fail, too.\n3. Each one fires an alert, you get swamped.\n\n### Why?\n- No alert grouping/deduplication, so you get one alert per service per pod.\n- Downstream failures avalanche from the real root cause.\n\n### Quick checks\n- Timeline: which alert came first?\n- Any recent deploy or config change?\n- Focus on user symptoms first.\n\n### Quick fixes\n- Silence duplicate alerts.\n- Roll back latest change if needed.\n- Triage: fix what hits users, not just what’s red.\n\n### Prevent this next time\n- Root-cause alert grouping.\n- Write recovery playbooks/runbooks.\n- Blameless postmortems after each storm.\n\n---\n\n## 🚦 \"App’s memory keeps rising — OOM killed\"\n\n**Symptom:**\n> App runs fine, then restarts. Memory usage graph only goes up. `OOMKilled` in Kubernetes.\n\n### Where to check\n- Container/pod status for restarts or OOM reason.\n- Memory profiles/heap dumps.\n- Any feature rolled out using more memory?\n\n### What’s really happening?\n1. App creates objects/data, doesn’t release old ones.\n2. GC can’t free enough space.\n3. OS kills process for hitting memory limit.\n\n### Why?\n- Memory leak (long-lived refs).\n- Cache grows endlessly.\n- Unclosed file handles, sockets.\n\n### Quick checks\n- `kubectl top pod`\n- App exposes heap profile (e.g. `/debug/pprof` Go, VisualVM–Java)\n- See what uses most RAM.\n\n### Quick fixes\n- Restart service (buy time).\n- Cap cache size or TTL.\n- Fix leaks, close all resources.\n- Raise memory limits (short-term).\n\n### Prevent next time\n- Test with profilers pre-prod.\n- Add dashboards & alerts for leak trends.\n- Enforce resource cleanup with automated/static checking.\n\n---\n\n## 🚦 \"Random DB CPU spikes, app slows to a crawl\"\n\n**Symptom:**\n> Database CPU spikes to 90%, queries slow way down. No obvious changes — except, did you just ship a big new feature?\n\n### Where to check\n- DB CPU, query, and connection metrics.\n- Recent code deploys?\n- Batch/report jobs during peak?\n- App’s DB pool usage?\n\n### What’s really happening?\n1. New query hits DB, scans big table.\n2. More user traffic = more bad queries.\n3. Everything backs up, users timeout.\n\n### Why?\n- Missing index.\n- Slow JOINs/filters.\n- Batch/report jobs.\n\n### Quick checks\n- `SHOW FULL PROCESSLIST` (MySQL)\n- `EXPLAIN SELECT ...`\n- Check pool/slow query dashboard.\n\n### Quick fixes\n- Add index, kill stuck queries.\n- Temp: scale up DB.\n- Schedule heavy jobs off-hours.\n\n### Prevent next time\n- Alerts on slow queries.\n- Load test DB after code changes.\n- Cap timeouts/connections.\n\n---\n\n## 🚦 \"Only users in Asia are timing out\"\n\n**Symptom:**\n> Users in one country have errors, everyone else is fine.\n\n### Where to check\n- DNS/CDN routes.\n- Per-region LB or healthcheck status.\n- Test from VPN/tools in that region.\n\n### What’s really happening?\n1. DNS routes Asia users to `asia-server`.\n2. That server’s DB is down. US/EU fine.\n3. Only Asia reports issues.\n\n### Why?\n- Edge server misconfigured.\n- Cloud region down.\n- DNS cache poisoning or wrong routes\n\n### Quick checks\n- `dig yourapp.com`\n- `traceroute yourapp.com`\n- Regional dashboard per endpoint.\n\n### Quick fixes\n- Reroute traffic to healthy region.\n- Restart broken services.\n- Invalidate DNS/CDN caches.\n\n### Prevent next time\n- Alerts + dashboards by region/country.\n- Regular synthetic tests around the world.\n- Failover plans ready.\n\n---\n\n# 🌟 Final Takeaway\n\nIncidents are just your system’s way of teaching you. Every error, crash, or slowdown is a story. Calmly follow the symptoms, check the steps, fix the problem — and you’ll get better at it every time.\n"}

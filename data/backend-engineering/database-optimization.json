{
  "title": "Database Optimization Techniques for Backend Engineers",
  "slug": "database-optimization",
  "date": "2025-01-16",
  "author": "Pass Gen",
  "tags": ["Backend Engineering", "Database", "Performance", "Optimization"],
  "excerpt": "Master database optimization techniques including indexing, query optimization, connection pooling, and caching strategies for high-performance applications.",
  "content": "# Database Optimization Techniques for Backend Engineers\n\nDatabase performance is critical for backend applications. This comprehensive guide covers essential optimization techniques to improve query performance, reduce latency, and scale your database effectively.\n\n## Indexing Strategies\n\n### Primary Indexes\n\n```sql\n-- Primary key (automatically indexed)\nCREATE TABLE users (\n  id SERIAL PRIMARY KEY,\n  email VARCHAR(255) UNIQUE NOT NULL,\n  name VARCHAR(255) NOT NULL,\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Unique index for email lookups\nCREATE UNIQUE INDEX idx_users_email ON users(email);\n```\n\n### Composite Indexes\n\n```sql\n-- Multi-column index for complex queries\nCREATE INDEX idx_orders_user_date ON orders(user_id, created_at DESC);\n\n-- Query that benefits from this index\nSELECT * FROM orders \nWHERE user_id = 123 \nORDER BY created_at DESC \nLIMIT 10;\n```\n\n### Partial Indexes\n\n```sql\n-- Index only active users\nCREATE INDEX idx_users_active ON users(email) \nWHERE status = 'active';\n\n-- Index only recent orders\nCREATE INDEX idx_orders_recent ON orders(user_id, created_at) \nWHERE created_at > '2024-01-01';\n```\n\n### Covering Indexes\n\n```sql\n-- Include all needed columns in index\nCREATE INDEX idx_users_covering ON users(email, name, status) \nINCLUDE (created_at, updated_at);\n\n-- Query can be satisfied entirely from index\nSELECT email, name, status, created_at \nFROM users \nWHERE email = 'user@example.com';\n```\n\n## Query Optimization\n\n### Use EXPLAIN to Analyze Queries\n\n```sql\n-- Analyze query execution plan\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT u.name, o.total \nFROM users u \nJOIN orders o ON u.id = o.user_id \nWHERE u.status = 'active' \nAND o.created_at > '2024-01-01';\n```\n\n### Optimize JOIN Operations\n\n```sql\n-- Use appropriate JOIN types\n-- INNER JOIN for matching records only\nSELECT u.name, o.total\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id\nWHERE u.status = 'active';\n\n-- LEFT JOIN to include all users\nSELECT u.name, COALESCE(SUM(o.total), 0) as total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n```\n\n### Avoid N+1 Queries\n\n```javascript\n// Bad: N+1 query problem\nconst users = await User.findAll();\nfor (const user of users) {\n  const orders = await Order.findAll({ where: { userId: user.id } });\n  user.orders = orders;\n}\n\n// Good: Single query with JOIN\nconst users = await User.findAll({\n  include: [{\n    model: Order,\n    as: 'orders'\n  }]\n});\n\n// Good: Batch loading\nconst userIds = users.map(u => u.id);\nconst allOrders = await Order.findAll({\n  where: { userId: { [Op.in]: userIds } }\n});\n// Group orders by userId\n```\n\n### Use LIMIT and OFFSET Efficiently\n\n```sql\n-- Use cursor-based pagination for large datasets\n-- Instead of OFFSET (slow for large offsets)\nSELECT * FROM orders \nWHERE id > 1000 \nORDER BY id \nLIMIT 20;\n\n-- Or use timestamp-based pagination\nSELECT * FROM orders \nWHERE created_at < '2024-01-15 10:30:00' \nORDER BY created_at DESC \nLIMIT 20;\n```\n\n## Connection Pooling\n\n### Node.js with pg-pool\n\n```javascript\nconst { Pool } = require('pg');\n\nconst pool = new Pool({\n  user: process.env.DB_USER,\n  host: process.env.DB_HOST,\n  database: process.env.DB_NAME,\n  password: process.env.DB_PASSWORD,\n  port: 5432,\n  max: 20, // Maximum number of clients in the pool\n  idleTimeoutMillis: 30000, // Close idle clients after 30 seconds\n  connectionTimeoutMillis: 2000, // Return an error after 2 seconds if connection could not be established\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false\n});\n\n// Use pool for queries\nasync function getUserById(id) {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT * FROM users WHERE id = $1', [id]);\n    return result.rows[0];\n  } finally {\n    client.release();\n  }\n}\n```\n\n### Connection Pool Configuration\n\n```javascript\n// Optimal pool settings based on application needs\nconst poolConfig = {\n  // For read-heavy applications\n  readPool: {\n    max: 50,\n    min: 5,\n    acquireTimeoutMillis: 30000,\n    idleTimeoutMillis: 60000\n  },\n  \n  // For write-heavy applications\n  writePool: {\n    max: 20,\n    min: 2,\n    acquireTimeoutMillis: 30000,\n    idleTimeoutMillis: 30000\n  }\n};\n```\n\n## Caching Strategies\n\n### Application-Level Caching\n\n```javascript\nconst NodeCache = require('node-cache');\nconst cache = new NodeCache({ stdTTL: 600 }); // 10 minutes\n\nasync function getCachedUser(userId) {\n  const cacheKey = `user:${userId}`;\n  \n  // Try cache first\n  let user = cache.get(cacheKey);\n  if (user) {\n    return user;\n  }\n  \n  // Fetch from database\n  user = await getUserFromDB(userId);\n  \n  // Store in cache\n  cache.set(cacheKey, user);\n  \n  return user;\n}\n```\n\n### Redis Caching\n\n```javascript\nconst redis = require('redis');\nconst client = redis.createClient({\n  host: process.env.REDIS_HOST,\n  port: process.env.REDIS_PORT,\n  password: process.env.REDIS_PASSWORD\n});\n\nasync function getCachedData(key) {\n  try {\n    const cached = await client.get(key);\n    return cached ? JSON.parse(cached) : null;\n  } catch (error) {\n    console.error('Cache error:', error);\n    return null;\n  }\n}\n\nasync function setCachedData(key, data, ttl = 3600) {\n  try {\n    await client.setex(key, ttl, JSON.stringify(data));\n  } catch (error) {\n    console.error('Cache set error:', error);\n  }\n}\n\n// Cache invalidation patterns\nasync function invalidateUserCache(userId) {\n  const patterns = [\n    `user:${userId}`,\n    `user:${userId}:orders`,\n    `user:${userId}:profile`\n  ];\n  \n  for (const pattern of patterns) {\n    await client.del(pattern);\n  }\n}\n```\n\n## Database Schema Optimization\n\n### Normalization vs Denormalization\n\n```sql\n-- Normalized schema (3NF)\nCREATE TABLE users (\n  id SERIAL PRIMARY KEY,\n  name VARCHAR(255) NOT NULL,\n  email VARCHAR(255) UNIQUE NOT NULL\n);\n\nCREATE TABLE user_profiles (\n  user_id INTEGER PRIMARY KEY REFERENCES users(id),\n  bio TEXT,\n  avatar_url VARCHAR(500),\n  location VARCHAR(255)\n);\n\n-- Denormalized for read performance\nCREATE TABLE user_summary (\n  id SERIAL PRIMARY KEY,\n  name VARCHAR(255) NOT NULL,\n  email VARCHAR(255) UNIQUE NOT NULL,\n  bio TEXT,\n  avatar_url VARCHAR(500),\n  location VARCHAR(255),\n  total_orders INTEGER DEFAULT 0,\n  last_order_date TIMESTAMP\n);\n```\n\n### Partitioning Large Tables\n\n```sql\n-- Partition by date range\nCREATE TABLE orders (\n  id SERIAL,\n  user_id INTEGER NOT NULL,\n  total DECIMAL(10,2) NOT NULL,\n  created_at TIMESTAMP NOT NULL\n) PARTITION BY RANGE (created_at);\n\n-- Create monthly partitions\nCREATE TABLE orders_2024_01 PARTITION OF orders\nFOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\nCREATE TABLE orders_2024_02 PARTITION OF orders\nFOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n```\n\n## Monitoring and Profiling\n\n### Query Performance Monitoring\n\n```javascript\n// Log slow queries\nconst slowQueryThreshold = 1000; // 1 second\n\nfunction logSlowQuery(query, duration) {\n  if (duration > slowQueryThreshold) {\n    console.warn(`Slow query detected: ${duration}ms`);\n    console.log('Query:', query);\n  }\n}\n\n// Wrap database calls\nasync function executeQuery(query, params) {\n  const start = Date.now();\n  try {\n    const result = await pool.query(query, params);\n    const duration = Date.now() - start;\n    logSlowQuery(query, duration);\n    return result;\n  } catch (error) {\n    const duration = Date.now() - start;\n    console.error(`Query failed after ${duration}ms:`, error);\n    throw error;\n  }\n}\n```\n\n### Database Metrics\n\n```javascript\n// Monitor connection pool metrics\nsetInterval(() => {\n  const poolStats = {\n    totalCount: pool.totalCount,\n    idleCount: pool.idleCount,\n    waitingCount: pool.waitingCount\n  };\n  \n  console.log('Pool stats:', poolStats);\n  \n  // Alert if pool is exhausted\n  if (poolStats.waitingCount > 10) {\n    console.warn('High connection pool wait time!');\n  }\n}, 30000); // Every 30 seconds\n```\n\n## Read Replicas\n\n```javascript\n// Separate read and write connections\nconst writePool = new Pool({\n  host: process.env.DB_WRITE_HOST,\n  // ... other config\n});\n\nconst readPool = new Pool({\n  host: process.env.DB_READ_HOST,\n  // ... other config\n});\n\n// Route queries appropriately\nasync function getUserById(id, useReadReplica = true) {\n  const pool = useReadReplica ? readPool : writePool;\n  const result = await pool.query('SELECT * FROM users WHERE id = $1', [id]);\n  return result.rows[0];\n}\n\nasync function createUser(userData) {\n  // Always use write pool for mutations\n  const result = await writePool.query(\n    'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',\n    [userData.name, userData.email]\n  );\n  return result.rows[0];\n}\n```\n\n## Best Practices Summary\n\n1. **Index strategically** - Create indexes for frequently queried columns\n2. **Monitor performance** - Use EXPLAIN and profiling tools\n3. **Optimize queries** - Avoid N+1 problems and use appropriate JOINs\n4. **Implement caching** - Cache frequently accessed data\n5. **Use connection pooling** - Manage database connections efficiently\n6. **Consider partitioning** - For very large tables\n7. **Use read replicas** - Scale read operations\n8. **Monitor and alert** - Set up proper monitoring\n9. **Regular maintenance** - Update statistics and rebuild indexes\n10. **Test with realistic data** - Use production-like data for testing\n\n## Conclusion\n\nDatabase optimization is an ongoing process that requires monitoring, testing, and continuous improvement. Start with the basics like proper indexing and query optimization, then move to more advanced techniques like caching and read replicas as your application scales.\n\nRemember: Premature optimization is the root of all evil, but ignoring performance until it becomes a problem is equally dangerous. Find the right balance for your specific use case."
}

{
  "title": "Vector Databases for AI Applications",
  "slug": "vector-databases",
  "date": "2025-01-17",
  "author": "Pass Gen",
  "tags": ["AI", "Vector Databases", "Embeddings", "Similarity Search", "Machine Learning"],
  "excerpt": "Complete guide to vector databases, embeddings, and similarity search for building AI-powered applications with semantic search capabilities.",
  "content": "# Vector Databases for AI Applications\n\nVector databases are specialized databases designed to store and query high-dimensional vectors efficiently. They're essential for AI applications that need semantic search, recommendation systems, and similarity matching.\n\n## Understanding Vector Embeddings\n\n### What are Vector Embeddings?\n\nVector embeddings are numerical representations of data (text, images, audio) in a high-dimensional space where similar items are closer together.\n\n```javascript\n// Example: Text embeddings\nconst textEmbeddings = {\n  'cat': [0.1, 0.3, -0.2, 0.8, ...], // 1536 dimensions\n  'dog': [0.2, 0.4, -0.1, 0.7, ...],\n  'car': [-0.3, 0.1, 0.9, -0.2, ...]\n};\n\n// Similar items have similar vectors\n// cat and dog are closer than cat and car\n```\n\n### Generating Embeddings with OpenAI\n\n```javascript\nconst OpenAI = require('openai');\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY\n});\n\nclass EmbeddingService {\n  constructor() {\n    this.model = 'text-embedding-3-small';\n    this.dimensions = 1536;\n  }\n  \n  async generateEmbedding(text) {\n    try {\n      const response = await openai.embeddings.create({\n        model: this.model,\n        input: text,\n        encoding_format: 'float'\n      });\n      \n      return response.data[0].embedding;\n    } catch (error) {\n      console.error('Embedding generation error:', error);\n      throw new Error('Failed to generate embedding');\n    }\n  }\n  \n  async generateBatchEmbeddings(texts) {\n    try {\n      const response = await openai.embeddings.create({\n        model: this.model,\n        input: texts,\n        encoding_format: 'float'\n      });\n      \n      return response.data.map(item => item.embedding);\n    } catch (error) {\n      console.error('Batch embedding error:', error);\n      throw new Error('Failed to generate batch embeddings');\n    }\n  }\n  \n  // Calculate cosine similarity\n  cosineSimilarity(vecA, vecB) {\n    if (vecA.length !== vecB.length) {\n      throw new Error('Vectors must have the same length');\n    }\n    \n    let dotProduct = 0;\n    let normA = 0;\n    let normB = 0;\n    \n    for (let i = 0; i < vecA.length; i++) {\n      dotProduct += vecA[i] * vecB[i];\n      normA += vecA[i] * vecA[i];\n      normB += vecB[i] * vecB[i];\n    }\n    \n    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n  }\n}\n\nconst embeddingService = new EmbeddingService();\n```\n\n## Vector Database Options\n\n### 1. Pinecone Integration\n\n```javascript\nconst { Pinecone } = require('@pinecone-database/pinecone');\n\nclass PineconeService {\n  constructor() {\n    this.pinecone = new Pinecone({\n      apiKey: process.env.PINECONE_API_KEY\n    });\n    this.indexName = 'ai-embeddings';\n  }\n  \n  async initialize() {\n    this.index = this.pinecone.index(this.indexName);\n  }\n  \n  async upsertVectors(vectors) {\n    try {\n      const upsertRequest = {\n        vectors: vectors.map(vector => ({\n          id: vector.id,\n          values: vector.embedding,\n          metadata: vector.metadata\n        }))\n      };\n      \n      const response = await this.index.upsert(upsertRequest);\n      return response;\n    } catch (error) {\n      console.error('Pinecone upsert error:', error);\n      throw error;\n    }\n  }\n  \n  async search(queryEmbedding, options = {}) {\n    try {\n      const searchRequest = {\n        vector: queryEmbedding,\n        topK: options.topK || 10,\n        includeMetadata: true,\n        includeValues: false,\n        filter: options.filter || {}\n      };\n      \n      const response = await this.index.query(searchRequest);\n      return response.matches;\n    } catch (error) {\n      console.error('Pinecone search error:', error);\n      throw error;\n    }\n  }\n  \n  async deleteVectors(ids) {\n    try {\n      await this.index.deleteMany(ids);\n    } catch (error) {\n      console.error('Pinecone delete error:', error);\n      throw error;\n    }\n  }\n}\n\nconst pineconeService = new PineconeService();\n```\n\n### 2. Weaviate Integration\n\n```javascript\nconst weaviate = require('weaviate-client');\n\nclass WeaviateService {\n  constructor() {\n    this.client = weaviate.client({\n      scheme: 'https',\n      host: process.env.WEAVIATE_HOST,\n      apiKey: new weaviate.ApiKey(process.env.WEAVIATE_API_KEY)\n    });\n    this.className = 'Document';\n  }\n  \n  async createSchema() {\n    const classDefinition = {\n      class: this.className,\n      vectorizer: 'text2vec-openai',\n      moduleConfig: {\n        'text2vec-openai': {\n          model: 'ada',\n          modelVersion: '002',\n          type: 'text'\n        }\n      },\n      properties: [\n        {\n          name: 'content',\n          dataType: ['text'],\n          description: 'The content of the document'\n        },\n        {\n          name: 'title',\n          dataType: ['string'],\n          description: 'The title of the document'\n        },\n        {\n          name: 'category',\n          dataType: ['string'],\n          description: 'The category of the document'\n        }\n      ]\n    };\n    \n    try {\n      await this.client.schema.classCreator().withClass(classDefinition).do();\n      console.log('Schema created successfully');\n    } catch (error) {\n      if (error.message.includes('already exists')) {\n        console.log('Schema already exists');\n      } else {\n        throw error;\n      }\n    }\n  }\n  \n  async addDocument(document) {\n    try {\n      const result = await this.client.data\n        .creator()\n        .withClass(this.className)\n        .withProperties({\n          content: document.content,\n          title: document.title,\n          category: document.category\n        })\n        .do();\n      \n      return result;\n    } catch (error) {\n      console.error('Weaviate add document error:', error);\n      throw error;\n    }\n  }\n  \n  async searchDocuments(query, options = {}) {\n    try {\n      const result = await this.client.graphql\n        .get()\n        .withClassName(this.className)\n        .withFields('content title category _additional { id distance }')\n        .withNearText({\n          concepts: [query],\n          certainty: options.certainty || 0.7\n        })\n        .withLimit(options.limit || 10)\n        .do();\n      \n      return result.data.Get[this.className];\n    } catch (error) {\n      console.error('Weaviate search error:', error);\n      throw error;\n    }\n  }\n}\n\nconst weaviateService = new WeaviateService();\n```\n\n### 3. Chroma Integration\n\n```javascript\nconst { ChromaClient } = require('chromadb');\n\nclass ChromaService {\n  constructor() {\n    this.client = new ChromaClient({\n      path: process.env.CHROMA_URL || 'http://localhost:8000'\n    });\n    this.collectionName = 'documents';\n  }\n  \n  async initialize() {\n    try {\n      this.collection = await this.client.getOrCreateCollection({\n        name: this.collectionName,\n        metadata: { description: 'Document embeddings collection' }\n      });\n    } catch (error) {\n      console.error('Chroma initialization error:', error);\n      throw error;\n    }\n  }\n  \n  async addDocuments(documents) {\n    try {\n      const ids = documents.map((_, index) => `doc_${Date.now()}_${index}`);\n      const texts = documents.map(doc => doc.content);\n      const metadatas = documents.map(doc => ({\n        title: doc.title,\n        category: doc.category,\n        source: doc.source\n      }));\n      \n      await this.collection.add({\n        ids,\n        documents: texts,\n        metadatas\n      });\n      \n      return ids;\n    } catch (error) {\n      console.error('Chroma add documents error:', error);\n      throw error;\n    }\n  }\n  \n  async search(query, options = {}) {\n    try {\n      const results = await this.collection.query({\n        queryTexts: [query],\n        nResults: options.limit || 10,\n        where: options.filter || {}\n      });\n      \n      return results;\n    } catch (error) {\n      console.error('Chroma search error:', error);\n      throw error;\n    }\n  }\n}\n\nconst chromaService = new ChromaService();\n```\n\n## Building a Semantic Search System\n\n### Document Processing Pipeline\n\n```javascript\nclass DocumentProcessor {\n  constructor(embeddingService, vectorDB) {\n    this.embeddingService = embeddingService;\n    this.vectorDB = vectorDB;\n  }\n  \n  async processDocument(document) {\n    try {\n      // Clean and preprocess text\n      const cleanedText = this.preprocessText(document.content);\n      \n      // Generate embedding\n      const embedding = await this.embeddingService.generateEmbedding(cleanedText);\n      \n      // Prepare vector data\n      const vectorData = {\n        id: document.id || `doc_${Date.now()}`,\n        embedding,\n        metadata: {\n          title: document.title,\n          content: cleanedText,\n          category: document.category,\n          source: document.source,\n          createdAt: new Date().toISOString()\n        }\n      };\n      \n      // Store in vector database\n      await this.vectorDB.upsertVectors([vectorData]);\n      \n      return vectorData;\n    } catch (error) {\n      console.error('Document processing error:', error);\n      throw error;\n    }\n  }\n  \n  preprocessText(text) {\n    return text\n      .toLowerCase()\n      .replace(/[^\\w\\s]/g, ' ') // Remove special characters\n      .replace(/\\s+/g, ' ') // Normalize whitespace\n      .trim();\n  }\n  \n  async processBatch(documents) {\n    const results = [];\n    \n    for (const document of documents) {\n      try {\n        const result = await this.processDocument(document);\n        results.push(result);\n      } catch (error) {\n        console.error(`Failed to process document ${document.id}:`, error);\n        results.push({ id: document.id, error: error.message });\n      }\n    }\n    \n    return results;\n  }\n}\n\nconst documentProcessor = new DocumentProcessor(embeddingService, pineconeService);\n```\n\n### Semantic Search API\n\n```javascript\nconst express = require('express');\nconst app = express();\n\n// Search endpoint\napp.post('/api/search', async (req, res) => {\n  try {\n    const { query, filters = {}, limit = 10 } = req.body;\n    \n    if (!query) {\n      return res.status(400).json({\n        error: 'Query is required'\n      });\n    }\n    \n    // Generate query embedding\n    const queryEmbedding = await embeddingService.generateEmbedding(query);\n    \n    // Search vector database\n    const results = await pineconeService.search(queryEmbedding, {\n      topK: limit,\n      filter: filters\n    });\n    \n    // Format results\n    const formattedResults = results.map(match => ({\n      id: match.id,\n      score: match.score,\n      title: match.metadata.title,\n      content: match.metadata.content,\n      category: match.metadata.category,\n      source: match.metadata.source\n    }));\n    \n    res.json({\n      success: true,\n      query,\n      results: formattedResults,\n      total: formattedResults.length\n    });\n  } catch (error) {\n    console.error('Search error:', error);\n    res.status(500).json({\n      error: 'Search failed'\n    });\n  }\n});\n\n// Add document endpoint\napp.post('/api/documents', async (req, res) => {\n  try {\n    const { documents } = req.body;\n    \n    if (!documents || !Array.isArray(documents)) {\n      return res.status(400).json({\n        error: 'Documents array is required'\n      });\n    }\n    \n    const results = await documentProcessor.processBatch(documents);\n    \n    res.json({\n      success: true,\n      processed: results.length,\n      results\n    });\n  } catch (error) {\n    console.error('Document processing error:', error);\n    res.status(500).json({\n      error: 'Document processing failed'\n    });\n  }\n});\n```\n\n## Advanced Vector Operations\n\n### Hybrid Search (Vector + Keyword)\n\n```javascript\nclass HybridSearchService {\n  constructor(vectorDB, textSearchDB) {\n    this.vectorDB = vectorDB;\n    this.textSearchDB = textSearchDB;\n  }\n  \n  async hybridSearch(query, options = {}) {\n    const { vectorWeight = 0.7, textWeight = 0.3 } = options;\n    \n    // Vector search\n    const queryEmbedding = await embeddingService.generateEmbedding(query);\n    const vectorResults = await this.vectorDB.search(queryEmbedding, {\n      topK: options.limit || 20\n    });\n    \n    // Text search (using PostgreSQL full-text search)\n    const textResults = await this.textSearchDB.query(\n      `SELECT id, title, content, ts_rank(to_tsvector('english', content), plainto_tsquery('english', $1)) as rank\n       FROM documents \n       WHERE to_tsvector('english', content) @@ plainto_tsquery('english', $1)\n       ORDER BY rank DESC\n       LIMIT $2`,\n      [query, options.limit || 20]\n    );\n    \n    // Combine and score results\n    const combinedResults = this.combineResults(\n      vectorResults,\n      textResults.rows,\n      vectorWeight,\n      textWeight\n    );\n    \n    return combinedResults\n      .sort((a, b) => b.score - a.score)\n      .slice(0, options.limit || 10);\n  }\n  \n  combineResults(vectorResults, textResults, vectorWeight, textWeight) {\n    const resultMap = new Map();\n    \n    // Add vector results\n    vectorResults.forEach(result => {\n      resultMap.set(result.id, {\n        id: result.id,\n        title: result.metadata.title,\n        content: result.metadata.content,\n        vectorScore: result.score,\n        textScore: 0,\n        combinedScore: result.score * vectorWeight\n      });\n    });\n    \n    // Add text results\n    textResults.forEach(result => {\n      const existing = resultMap.get(result.id);\n      if (existing) {\n        existing.textScore = result.rank;\n        existing.combinedScore += result.rank * textWeight;\n      } else {\n        resultMap.set(result.id, {\n          id: result.id,\n          title: result.title,\n          content: result.content,\n          vectorScore: 0,\n          textScore: result.rank,\n          combinedScore: result.rank * textWeight\n        });\n      }\n    });\n    \n    return Array.from(resultMap.values());\n  }\n}\n```\n\n### Vector Clustering and Categorization\n\n```javascript\nclass VectorClusteringService {\n  constructor(vectorDB) {\n    this.vectorDB = vectorDB;\n  }\n  \n  async clusterDocuments(options = {}) {\n    const { k = 5, minClusterSize = 3 } = options;\n    \n    // Get all vectors from database\n    const allVectors = await this.getAllVectors();\n    \n    if (allVectors.length < k) {\n      throw new Error('Not enough documents for clustering');\n    }\n    \n    // Simple K-means clustering\n    const clusters = await this.kMeansClustering(allVectors, k);\n    \n    // Filter clusters by minimum size\n    const validClusters = clusters.filter(\n      cluster => cluster.documents.length >= minClusterSize\n    );\n    \n    return validClusters;\n  }\n  \n  async kMeansClustering(vectors, k) {\n    // Initialize centroids randomly\n    const centroids = this.initializeCentroids(vectors, k);\n    \n    let clusters = [];\n    let iterations = 0;\n    const maxIterations = 100;\n    \n    while (iterations < maxIterations) {\n      // Assign vectors to nearest centroid\n      clusters = this.assignToClusters(vectors, centroids);\n      \n      // Update centroids\n      const newCentroids = this.updateCentroids(clusters);\n      \n      // Check for convergence\n      if (this.hasConverged(centroids, newCentroids)) {\n        break;\n      }\n      \n      centroids = newCentroids;\n      iterations++;\n    }\n    \n    return clusters;\n  }\n  \n  initializeCentroids(vectors, k) {\n    const centroids = [];\n    const usedIndices = new Set();\n    \n    while (centroids.length < k) {\n      const randomIndex = Math.floor(Math.random() * vectors.length);\n      if (!usedIndices.has(randomIndex)) {\n        centroids.push([...vectors[randomIndex].embedding]);\n        usedIndices.add(randomIndex);\n      }\n    }\n    \n    return centroids;\n  }\n  \n  assignToClusters(vectors, centroids) {\n    const clusters = centroids.map(() => ({ documents: [], centroid: null }));\n    \n    vectors.forEach(vector => {\n      let minDistance = Infinity;\n      let closestCluster = 0;\n      \n      centroids.forEach((centroid, index) => {\n        const distance = this.euclideanDistance(vector.embedding, centroid);\n        if (distance < minDistance) {\n          minDistance = distance;\n          closestCluster = index;\n        }\n      });\n      \n      clusters[closestCluster].documents.push(vector);\n    });\n    \n    return clusters;\n  }\n  \n  updateCentroids(clusters) {\n    return clusters.map(cluster => {\n      if (cluster.documents.length === 0) {\n        return cluster.centroid || [];\n      }\n      \n      const dimension = cluster.documents[0].embedding.length;\n      const newCentroid = new Array(dimension).fill(0);\n      \n      cluster.documents.forEach(doc => {\n        doc.embedding.forEach((value, index) => {\n          newCentroid[index] += value;\n        });\n      });\n      \n      return newCentroid.map(value => value / cluster.documents.length);\n    });\n  }\n  \n  euclideanDistance(vecA, vecB) {\n    let sum = 0;\n    for (let i = 0; i < vecA.length; i++) {\n      sum += Math.pow(vecA[i] - vecB[i], 2);\n    }\n    return Math.sqrt(sum);\n  }\n  \n  hasConverged(oldCentroids, newCentroids, threshold = 0.001) {\n    for (let i = 0; i < oldCentroids.length; i++) {\n      const distance = this.euclideanDistance(oldCentroids[i], newCentroids[i]);\n      if (distance > threshold) {\n        return false;\n      }\n    }\n    return true;\n  }\n}\n```\n\n## Performance Optimization\n\n### Batch Processing\n\n```javascript\nclass BatchProcessor {\n  constructor(embeddingService, vectorDB) {\n    this.embeddingService = embeddingService;\n    this.vectorDB = vectorDB;\n    this.batchSize = 100;\n    this.delay = 1000; // 1 second between batches\n  }\n  \n  async processBatch(documents) {\n    const results = [];\n    \n    for (let i = 0; i < documents.length; i += this.batchSize) {\n      const batch = documents.slice(i, i + this.batchSize);\n      \n      try {\n        // Generate embeddings for batch\n        const texts = batch.map(doc => doc.content);\n        const embeddings = await this.embeddingService.generateBatchEmbeddings(texts);\n        \n        // Prepare vector data\n        const vectors = batch.map((doc, index) => ({\n          id: doc.id || `doc_${Date.now()}_${i + index}`,\n          embedding: embeddings[index],\n          metadata: {\n            title: doc.title,\n            content: doc.content,\n            category: doc.category\n          }\n        }));\n        \n        // Upsert to vector database\n        await this.vectorDB.upsertVectors(vectors);\n        \n        results.push(...vectors);\n        \n        // Delay between batches to avoid rate limiting\n        if (i + this.batchSize < documents.length) {\n          await this.delay(this.delay);\n        }\n      } catch (error) {\n        console.error(`Batch ${i / this.batchSize + 1} failed:`, error);\n        results.push(...batch.map(doc => ({ id: doc.id, error: error.message })));\n      }\n    }\n    \n    return results;\n  }\n  \n  sleep(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n```\n\n## Conclusion\n\nVector databases are essential for building AI-powered applications that need:\n\n1. **Semantic Search** - Find similar content based on meaning\n2. **Recommendation Systems** - Suggest relevant items to users\n3. **Content Categorization** - Automatically organize content\n4. **Question Answering** - Find relevant information for queries\n5. **Similarity Matching** - Identify duplicate or similar items\n\nKey considerations when choosing a vector database:\n\n- **Scalability** - Can it handle your data volume?\n- **Performance** - How fast are queries?\n- **Features** - What operations does it support?\n- **Cost** - What's the pricing model?\n- **Integration** - How easy is it to integrate?\n\nStart with a simple implementation and gradually add complexity as your needs grow. Remember to monitor performance and costs as you scale your vector database usage."
}
